{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport random\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel, \n    AutoConfig,\n    ViTImageProcessor, \n    ViTModel,\n    get_linear_schedule_with_warmup\n)\n\n# Get a logger instance for the current module\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)  \n\nlog_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\nif not logger.handlers:\n    log_file_name = f\"vqa_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n    log_file_path = f\"/kaggle/working/{log_file_name}\"\n    \n    file_handler = logging.FileHandler(log_file_path)\n    file_handler.setLevel(logging.INFO)  \n    file_handler.setFormatter(log_formatter)\n    logger.addHandler(file_handler)\n\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.INFO)  \n    stream_handler.setFormatter(log_formatter)\n    logger.addHandler(stream_handler)\n    \n    logger.info(f\"Logging initialized. Log file will be at: {log_file_path}\")\nelse:\n    logger.info(\"Logger already initialized with handlers.\")\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"Using device: {device}\")\n\n# Config\nCONFIG = {\n    'data_dir': '/kaggle/input/vizwiz/',\n    'image_size': 384,\n    'batch_size': 16,\n    'num_workers': 4,\n    'learning_rate': 2e-5,\n    'weight_decay': 0.01,\n    'num_epochs': 10,\n    'warmup_ratio': 0.1,\n    'max_grad_norm': 1.0,\n    'text_model': 'bert-base-uncased',\n    'vision_model': 'google/vit-base-patch16-384',\n    'hidden_size': 768,\n    'dropout': 0.1,\n    'save_dir': './models',\n    'seed': 42\n}\n\n# Create save directory if it doesn't exist\nos.makedirs(CONFIG['save_dir'], exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VizWiz Dataset\nclass VizWizDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, processor, tokenizer, max_length=128, split='train'):\n        self.annotations = json.load(open(annotations_file, 'r'))\n        self.img_dir = img_dir\n        self.processor = processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.split = split\n        \n        # Create answer vocabulary from training data\n        if split == 'train':\n            self.answer_vocab = self._create_answer_vocab()\n            # Save vocab for inference\n            with open('answer_vocab.json', 'w') as f:\n                json.dump(self.answer_vocab, f)\n        else:\n            # Load vocab for val/test\n            try:\n                with open('answer_vocab.json', 'r') as f:\n                    self.answer_vocab = json.load(f)\n            except FileNotFoundError:\n                logger.warning(\"Answer vocabulary not found, creating from current split (not recommended)\")\n                self.answer_vocab = self._create_answer_vocab()\n                \n        logger.info(f\"Loaded {len(self.annotations)} samples for {split} split\")\n        logger.info(f\"Answer vocabulary size: {len(self.answer_vocab)}\")\n        \n    def _create_answer_vocab(self):\n        \"\"\"Create a vocabulary of answers and map them to indices\"\"\"\n        answers = []\n        for item in self.annotations:\n            # Get all answers with confidence \"yes\"\n            confident_answers = [a['answer'].lower() for a in item['answers'] \n                               if a['answer_confidence'] == 'yes']\n            if confident_answers:\n                # Use the most common answer\n                from collections import Counter\n                most_common = Counter(confident_answers).most_common(1)[0][0]\n                answers.append(most_common)\n        \n        # Add special tokens\n        special_tokens = ['unanswerable', 'unknown']\n        unique_answers = special_tokens + list(set(answers))\n        \n        # Create vocab dictionary\n        answer_to_idx = {ans: idx for idx, ans in enumerate(unique_answers)}\n        idx_to_answer = {idx: ans for idx, ans in enumerate(unique_answers)}\n        \n        return {\n            'answer_to_idx': answer_to_idx,\n            'idx_to_answer': idx_to_answer\n        }\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        item = self.annotations[idx]\n        \n        # Load and preprocess image\n        image_path = os.path.join(self.img_dir, item['image'])\n        try:\n            image = Image.open(image_path).convert('RGB')\n            image_encoding = self.processor(images=image, return_tensors=\"pt\")\n            image_encoding = {k: v.squeeze(0) for k, v in image_encoding.items()}\n        except Exception as e:\n            logger.error(f\"Error processing image {image_path}: {e}\")\n            # Use a black image as fallback\n            image = Image.new('RGB', (CONFIG['image_size'], CONFIG['image_size']), color=0)\n            image_encoding = self.processor(images=image, return_tensors=\"pt\")\n            image_encoding = {k: v.squeeze(0) for k, v in image_encoding.items()}\n        \n        # Process question\n        question = item['question']\n        question_encoding = self.tokenizer(\n            question,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        question_encoding = {k: v.squeeze(0) for k, v in question_encoding.items()}\n        \n        # Process answer\n        answerable = item['answerable']\n        if answerable == 0:\n            answer_idx = self.answer_vocab['answer_to_idx']['unanswerable']\n        else:\n            # Get most frequent answer with 'yes' confidence\n            yes_answers = [a['answer'].lower() for a in item['answers'] \n                          if a['answer_confidence'] == 'yes']\n            \n            if yes_answers:\n                from collections import Counter\n                most_common = Counter(yes_answers).most_common(1)[0][0]\n                # If answer is not in vocabulary, use 'unknown'\n                answer_idx = self.answer_vocab['answer_to_idx'].get(\n                    most_common, \n                    self.answer_vocab['answer_to_idx']['unknown']\n                )\n            else:\n                answer_idx = self.answer_vocab['answer_to_idx']['unknown']\n                \n        return {\n            'image_encoding': image_encoding,\n            'question_encoding': question_encoding,\n            'answer_idx': torch.tensor(answer_idx, dtype=torch.long),\n            'answerable': torch.tensor(answerable, dtype=torch.long),\n            'image_id': item['image'],\n            'question': question\n        }\n\n# Data collator\ndef collate_fn(batch):\n    image_encodings = {\n        'pixel_values': torch.stack([item['image_encoding']['pixel_values'] for item in batch])\n    }\n    \n    input_ids = torch.stack([item['question_encoding']['input_ids'] for item in batch])\n    attention_mask = torch.stack([item['question_encoding']['attention_mask'] for item in batch])\n    \n    question_encodings = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask\n    }\n    \n    answer_idxs = torch.stack([item['answer_idx'] for item in batch])\n    answerable = torch.stack([item['answerable'] for item in batch])\n    \n    image_ids = [item['image_id'] for item in batch]\n    questions = [item['question'] for item in batch]\n    \n    return {\n        'image_encodings': image_encodings,\n        'question_encodings': question_encodings,\n        'answer_idxs': answer_idxs,\n        'answerable': answerable,\n        'image_ids': image_ids,\n        'questions': questions\n    }\n\n# Load data\ndef load_data(config):\n    # Initialize models for preprocessing\n    tokenizer = AutoTokenizer.from_pretrained(config['text_model'])\n    processor = ViTImageProcessor.from_pretrained(config['vision_model'])\n    \n    # Data paths\n    train_ann_path = os.path.join(config['data_dir'], 'Annotations/Annotations/train.json')\n    val_ann_path = os.path.join(config['data_dir'], 'Annotations/Annotations/val.json')\n    test_ann_path = os.path.join(config['data_dir'], 'Annotations/Annotations/test.json')\n    \n    train_img_dir = os.path.join(config['data_dir'], 'train/train/')\n    val_img_dir = os.path.join(config['data_dir'], 'val/val/')\n    test_img_dir = os.path.join(config['data_dir'], 'test/test/')\n    \n    # Create datasets\n    train_dataset = VizWizDataset(\n        train_ann_path, train_img_dir, processor, tokenizer, split='train'\n    )\n    val_dataset = VizWizDataset(\n        val_ann_path, val_img_dir, processor, tokenizer, split='val'\n    )\n    test_dataset = VizWizDataset(\n        test_ann_path, test_img_dir, processor, tokenizer, split='test'\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    \n    return train_loader, val_loader, test_loader, train_dataset.answer_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VQA Model\nclass VQAModel(nn.Module):\n    def __init__(self, config, num_answers):\n        super(VQAModel, self).__init__()\n        self.config = config\n        self.num_answers = num_answers\n        \n        # Vision encoder\n        self.vision_config = AutoConfig.from_pretrained(config['vision_model'])\n        self.vision_encoder = ViTModel.from_pretrained(config['vision_model'])\n        \n        # Text encoder\n        self.text_config = AutoConfig.from_pretrained(config['text_model'])\n        self.text_encoder = AutoModel.from_pretrained(config['text_model'])\n        \n        # Projection layers\n        self.vision_projection = nn.Linear(\n            self.vision_config.hidden_size, config['hidden_size']\n        )\n        self.text_projection = nn.Linear(\n            self.text_config.hidden_size, config['hidden_size']\n        )\n        \n        # Multimodal fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(2 * config['hidden_size'], config['hidden_size']),\n            nn.LayerNorm(config['hidden_size']),\n            nn.GELU(),\n            nn.Dropout(config['dropout'])\n        )\n        \n        # Answer prediction\n        self.classifier = nn.Sequential(\n            nn.Linear(config['hidden_size'], config['hidden_size']),\n            nn.LayerNorm(config['hidden_size']),\n            nn.GELU(),\n            nn.Dropout(config['dropout']),\n            nn.Linear(config['hidden_size'], num_answers)\n        )\n        \n        # Answerable prediction\n        self.answerable_classifier = nn.Sequential(\n            nn.Linear(config['hidden_size'], config['hidden_size'] // 2),\n            nn.LayerNorm(config['hidden_size'] // 2),\n            nn.GELU(),\n            nn.Dropout(config['dropout']),\n            nn.Linear(config['hidden_size'] // 2, 2)  # Binary classification\n        )\n        \n    def forward(self, image_encodings, question_encodings):\n        # Process image\n        vision_outputs = self.vision_encoder(**image_encodings)\n        vision_embeds = vision_outputs.last_hidden_state[:, 0]  # CLS token\n        vision_embeds = self.vision_projection(vision_embeds)\n        \n        # Process text\n        text_outputs = self.text_encoder(**question_encodings)\n        text_embeds = text_outputs.last_hidden_state[:, 0]  # CLS token\n        text_embeds = self.text_projection(text_embeds)\n        \n        # Combine modalities\n        multimodal_features = torch.cat([vision_embeds, text_embeds], dim=1)\n        fused_features = self.fusion(multimodal_features)\n        \n        # Predict answers and answerable\n        answer_logits = self.classifier(fused_features)\n        answerable_logits = self.answerable_classifier(fused_features)\n        \n        return {\n            'answer_logits': answer_logits,\n            'answerable_logits': answerable_logits,\n            'fused_features': fused_features\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training and evaluation functions\ndef train_one_epoch(model, data_loader, optimizer, scheduler, criterion, device, epoch, config):\n    model.train()\n    \n    losses = []\n    answer_preds = []\n    answer_targets = []\n    answerable_preds = []\n    answerable_targets = []\n    \n    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n    \n    for batch in progress_bar:\n        # Move data to device\n        image_encodings = {k: v.to(device) for k, v in batch['image_encodings'].items()}\n        question_encodings = {k: v.to(device) for k, v in batch['question_encodings'].items()}\n        answer_idxs = batch['answer_idxs'].to(device)\n        answerable = batch['answerable'].to(device)\n        \n        # Forward pass\n        outputs = model(image_encodings, question_encodings)\n        \n        # Calculate loss\n        answer_loss = criterion(outputs['answer_logits'], answer_idxs)\n        answerable_loss = criterion(outputs['answerable_logits'], answerable)\n        \n        # Combined loss\n        loss = answer_loss + answerable_loss\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        \n        # Record predictions and targets\n        answer_pred = torch.argmax(outputs['answer_logits'], dim=1)\n        answerable_pred = torch.argmax(outputs['answerable_logits'], dim=1)\n        \n        answer_preds.extend(answer_pred.detach().cpu().numpy())\n        answer_targets.extend(answer_idxs.detach().cpu().numpy())\n        answerable_preds.extend(answerable_pred.detach().cpu().numpy())\n        answerable_targets.extend(answerable.detach().cpu().numpy())\n        \n        # Update progress bar\n        losses.append(loss.item())\n        avg_loss = sum(losses) / len(losses)\n        progress_bar.set_postfix({\n            'loss': f\"{avg_loss:.4f}\",\n        })\n    \n    # Calculate metrics\n    answer_accuracy = np.mean(np.array(answer_preds) == np.array(answer_targets))\n    answerable_accuracy = np.mean(np.array(answerable_preds) == np.array(answerable_targets))\n    \n    return {\n        'loss': np.mean(losses),\n        'answer_accuracy': answer_accuracy,\n        'answerable_accuracy': answerable_accuracy\n    }\n\ndef evaluate(model, data_loader, criterion, device, epoch, config, split='Val'):\n    model.eval()\n    \n    losses = []\n    answer_preds = []\n    answer_targets = []\n    answerable_preds = []\n    answerable_targets = []\n    \n    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [{split}]\")\n    \n    with torch.no_grad():\n        for batch in progress_bar:\n            # Move data to device\n            image_encodings = {k: v.to(device) for k, v in batch['image_encodings'].items()}\n            question_encodings = {k: v.to(device) for k, v in batch['question_encodings'].items()}\n            answer_idxs = batch['answer_idxs'].to(device)\n            answerable = batch['answerable'].to(device)\n            \n            # Forward pass\n            outputs = model(image_encodings, question_encodings)\n            \n            # Calculate loss\n            answer_loss = criterion(outputs['answer_logits'], answer_idxs)\n            answerable_loss = criterion(outputs['answerable_logits'], answerable)\n            \n            # Combined loss\n            loss = answer_loss + answerable_loss\n            \n            # Record predictions and targets\n            answer_pred = torch.argmax(outputs['answer_logits'], dim=1)\n            answerable_pred = torch.argmax(outputs['answerable_logits'], dim=1)\n            \n            answer_preds.extend(answer_pred.detach().cpu().numpy())\n            answer_targets.extend(answer_idxs.detach().cpu().numpy())\n            answerable_preds.extend(answerable_pred.detach().cpu().numpy())\n            answerable_targets.extend(answerable.detach().cpu().numpy())\n            \n            # Update progress bar\n            losses.append(loss.item())\n            avg_loss = sum(losses) / len(losses)\n            progress_bar.set_postfix({\n                'loss': f\"{avg_loss:.4f}\",\n            })\n    \n    # Calculate metrics\n    answer_accuracy = np.mean(np.array(answer_preds) == np.array(answer_targets))\n    answerable_accuracy = np.mean(np.array(answerable_preds) == np.array(answerable_targets))\n    \n    return {\n        'loss': np.mean(losses),\n        'answer_accuracy': answer_accuracy,\n        'answerable_accuracy': answerable_accuracy\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main training function\ndef train_model(config):\n    logger.info(\"Starting model training...\")\n    logger.info(f\"Configuration: {config}\")\n    \n    # Load data\n    train_loader, val_loader, test_loader, answer_vocab = load_data(config)\n    num_answers = len(answer_vocab['answer_to_idx'])\n    logger.info(f\"Number of answers in vocabulary: {num_answers}\")\n    \n    # Initialize model\n    model = VQAModel(config, num_answers)\n    model.to(device)\n    \n    # Calculate trainable parameters\n    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"Model has {param_count:,} trainable parameters\")\n    \n    # Initialize optimizer and scheduler\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=config['learning_rate'],\n        weight_decay=config['weight_decay']\n    )\n    \n    total_steps = len(train_loader) * config['num_epochs']\n    warmup_steps = int(total_steps * config['warmup_ratio'])\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    best_val_accuracy = 0\n    metrics_history = {\n        'train_loss': [],\n        'train_answer_acc': [],\n        'train_answerable_acc': [],\n        'val_loss': [],\n        'val_answer_acc': [],\n        'val_answerable_acc': []\n    }\n    \n    for epoch in range(config['num_epochs']):\n        # Train\n        train_metrics = train_one_epoch(\n            model, train_loader, optimizer, scheduler, criterion, device, epoch, config\n        )\n        \n        # Validate\n        val_metrics = evaluate(\n            model, val_loader, criterion, device, epoch, config\n        )\n        \n        # Log metrics\n        logger.info(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n        logger.info(f\"Train Loss: {train_metrics['loss']:.4f}, \"\n                   f\"Answer Acc: {train_metrics['answer_accuracy']:.4f}, \"\n                   f\"Answerable Acc: {train_metrics['answerable_accuracy']:.4f}\")\n        logger.info(f\"Val Loss: {val_metrics['loss']:.4f}, \"\n                   f\"Answer Acc: {val_metrics['answer_accuracy']:.4f}, \"\n                   f\"Answerable Acc: {val_metrics['answerable_accuracy']:.4f}\")\n        \n        # Update metrics history\n        metrics_history['train_loss'].append(train_metrics['loss'])\n        metrics_history['train_answer_acc'].append(train_metrics['answer_accuracy'])\n        metrics_history['train_answerable_acc'].append(train_metrics['answerable_accuracy'])\n        metrics_history['val_loss'].append(val_metrics['loss'])\n        metrics_history['val_answer_acc'].append(val_metrics['answer_accuracy'])\n        metrics_history['val_answerable_acc'].append(val_metrics['answerable_accuracy'])\n        \n        # Save model if it's the best so far\n        val_accuracy = val_metrics['answer_accuracy']\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            \n            # Save model\n            model_path = os.path.join(config['save_dir'], f\"vqa_model_best.pt\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_accuracy': val_accuracy,\n                'config': config,\n                'answer_vocab': answer_vocab\n            }, model_path)\n            logger.info(f\"Saved best model with val accuracy: {val_accuracy:.4f}\")\n        \n        # Save checkpoint every epoch\n        checkpoint_path = os.path.join(config['save_dir'], f\"vqa_model_epoch_{epoch+1}.pt\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_accuracy': val_accuracy,\n            'config': config,\n            'answer_vocab': answer_vocab\n        }, checkpoint_path)\n    \n    # Plot training history\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 2, 1)\n    plt.plot(metrics_history['train_loss'], label='Train')\n    plt.plot(metrics_history['val_loss'], label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss')\n    \n    plt.subplot(2, 2, 2)\n    plt.plot(metrics_history['train_answer_acc'], label='Train')\n    plt.plot(metrics_history['val_answer_acc'], label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Answer Accuracy')\n    \n    plt.subplot(2, 2, 3)\n    plt.plot(metrics_history['train_answerable_acc'], label='Train')\n    plt.plot(metrics_history['val_answerable_acc'], label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Answerable Accuracy')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(config['save_dir'], 'training_history.png'))\n    \n    # Evaluate on test set\n    logger.info(\"Evaluating on test set...\")\n    \n    # Load best model\n    checkpoint = torch.load(os.path.join(config['save_dir'], f\"vqa_model_best.pt\"))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    test_metrics = evaluate(\n        model, test_loader, criterion, device, -1, config, split='Test'\n    )\n    \n    logger.info(f\"Test Loss: {test_metrics['loss']:.4f}, \"\n               f\"Answer Acc: {test_metrics['answer_accuracy']:.4f}, \"\n               f\"Answerable Acc: {test_metrics['answerable_accuracy']:.4f}\")\n    \n    return model, answer_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference function\ndef predict(model, image_path, question, processor, tokenizer, answer_vocab, device, config):\n    model.eval()\n    \n    # Preprocess image\n    image = Image.open(image_path).convert('RGB')\n    image_encoding = processor(images=image, return_tensors=\"pt\")\n    image_encoding = {k: v.to(device) for k, v in image_encoding.items()}\n    \n    # Preprocess question\n    question_encoding = tokenizer(\n        question,\n        padding='max_length',\n        truncation=True,\n        max_length=128,\n        return_tensors='pt'\n    )\n    question_encoding = {k: v.to(device) for k, v in question_encoding.items()}\n    \n    # Get predictions\n    with torch.no_grad():\n        outputs = model(image_encoding, question_encoding)\n        \n        answer_logits = outputs['answer_logits']\n        answerable_logits = outputs['answerable_logits']\n        \n        answer_idx = torch.argmax(answer_logits, dim=1).item()\n        answerable_idx = torch.argmax(answerable_logits, dim=1).item()\n        \n        answer = answer_vocab['idx_to_answer'][str(answer_idx)]\n        is_answerable = bool(answerable_idx)\n        \n        # Get confidence scores\n        answer_probs = torch.softmax(answer_logits, dim=1)[0]\n        answerable_probs = torch.softmax(answerable_logits, dim=1)[0]\n        \n        answer_confidence = answer_probs[answer_idx].item()\n        answerable_confidence = answerable_probs[answerable_idx].item()\n    \n    return {\n        'answer': answer,\n        'answer_confidence': answer_confidence,\n        'is_answerable': is_answerable,\n        'answerable_confidence': answerable_confidence\n    }\n\n# Visualization function for demo\ndef visualize_prediction(image_path, question, prediction):\n    plt.figure(figsize=(10, 8))\n    \n    image = Image.open(image_path).convert('RGB')\n    plt.imshow(image)\n    plt.axis('off')\n    \n    is_answerable = \"Yes\" if prediction['is_answerable'] else \"No\"\n    \n    plt.title(f\"Q: {question}\\n\"\n              f\"A: {prediction['answer']} (Confidence: {prediction['answer_confidence']:.2f})\\n\"\n              f\"Answerable: {is_answerable} (Confidence: {prediction['answerable_confidence']:.2f})\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the entire training pipeline\nif __name__ == \"__main__\":\n    # Train model\n    model, answer_vocab = train_model(CONFIG)\n    \n    # Save final model\n    final_model_path = os.path.join(CONFIG['save_dir'], \"vqa_model_final.pt\")\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'config': CONFIG,\n        'answer_vocab': answer_vocab\n    }, final_model_path)\n    logger.info(f\"Saved final model to {final_model_path}\")\n    \n    # # Example of how to load the model for inference\n    # logger.info(\"Loading model for inference...\")\n    \n    # # Initialize preprocessors\n    # processor = ViTImageProcessor.from_pretrained(CONFIG['vision_model'])\n    # tokenizer = AutoTokenizer.from_pretrained(CONFIG['text_model'])\n    \n    # # Load model\n    # checkpoint = torch.load(final_model_path)\n    # loaded_model = VQAModel(CONFIG, len(checkpoint['answer_vocab']['answer_to_idx']))\n    # loaded_model.load_state_dict(checkpoint['model_state_dict'])\n    # loaded_model.to(device)\n    \n    # logger.info(\"Model loaded successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}